model: "Tacotron 2"
sample_rate: &sr 22050
# <PAD>, <BOS>, <EOS> will be added by the tacotron2.py script
labels: &labels [' ', '!', '"', "'", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B',
                 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',
                 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b',
                 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',
                 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
n_fft: &n_fft 1024
n_mels: &n_mels 80
fmax: &fmax 8000
n_stride: &n_window_stride 256
pad_value: &pad_value -11.52

TranscriptDataLayer:
    labels: *labels
    batch_size: 32
    shuffle: false

Tacotron2Encoder:
    encoder_kernel_size: 5
    encoder_n_convolutions: 3
    encoder_embedding_dim: &enc_emb_dim 512

TextEmbedding:
    symbols_embedding_dim: *enc_emb_dim

Tacotron2Decoder:
    n_mel_channels: *n_mels
    n_frames_per_step: 1 # currently only 1 is supported
    encoder_embedding_dim: *enc_emb_dim
    decoder_rnn_dim: 1024
    prenet_dim: 256
    max_decoder_steps: 6000
    gate_threshold: 0.5
    p_attention_dropout: 0.1
    p_decoder_dropout: 0.1

    # Attention parameters
    attention_rnn_dim: 1024
    attention_dim: 128

    # Location Layer parameters
    attention_location_n_filters: 32
    attention_location_kernel_size: 31

Tacotron2Postnet:
    n_mel_channels: *n_mels
    postnet_embedding_dim: 512
    postnet_kernel_size: 5
    postnet_n_convolutions: 5

WaveGlowNM:
    n_window_stride: *n_window_stride
    n_mel_channels: *n_mels
    n_flows: 12
    n_group: 8
    n_early_every: 4
    n_early_size: 2
    n_wn_layers: 8
    n_wn_channels: 512
    sample_rate: *sr
    wn_kernel_size: 3

GriffinLim:
    sample_rate: *sr
    n_fft: *n_fft
    n_mels: *n_mels
    fmax: *fmax
